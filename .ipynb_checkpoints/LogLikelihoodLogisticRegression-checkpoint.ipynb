{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of MNIST digits using logistic regression\n",
    "**_Jake Daly | CSE 250A_**\n",
    "\n",
    "In this problem, we will classify some MNIST hand written digits (which have been mapped to an 8 pixel x 8 pixel image size) by training a logistic regression model. \n",
    "\n",
    "The network that we will use to probabilistically model whether a given image is a \"3\" or a \"5\" is a naive Bayes model with a single child node, and sixty-four parent nodes (each with a single connection directed at the child node). The parent nodes of this network represent each of sixty-four pixels in the images that we will use to train the network. The lone child node will be the binary output variable over {0, 1}, with a 0 representing a 3, and 1 representing a 5. The probability equation represented by this network is given by:\n",
    "\n",
    "![SegmentLocal](ProbYgivX.gif \"segment\")\n",
    "\n",
    "After we have learned some weights <code>w</code>, we can take the inner product of these weights with a data vector (a \"3\" or a \"5\"), and push this value through the sigmoid function (so that we obtain a valid probability over [0, 1]. Intuitively, when the inner product of our weights and a given image produces a large (positive) value, the sigmoid will activate and yield a probability of close to 1. When this inner product returns a small (negative) value, the sigmoid will not activate, and the probability that Y = 1 given this X will be close to 0. Thus, when a learned weight is more negative it will be suggestive that the node is a 3 when it receives an input of 1. Conversely, when a learned weight is a larger positive value, the sigmoid (when x=1) will yield a value closer to 1, suggesting that the x vector is more likely a 5 (solely based off of this single node)\n",
    "\n",
    "We will use the *log* *likelihood* probability to measure Y given X. To compute this as a function over both values of Y, we will need to marginilize over both values of Y. Because each sample is independently and identically distributed (IID), we can write this as:\n",
    "\n",
    "![SegmentLocal](LogProdEqSum.gif \"segment\")\n",
    "\n",
    "\n",
    "Which can be written as\n",
    "\n",
    "![SegmentLocal](LogLik.gif \"segment\")\n",
    "\n",
    "taking advantage of the fact that Y is a binary variable over {0, 1}.\n",
    "\n",
    "This is the log likelihood that we will apply gradient ascent to in order to train our algorithm (optimize <code>w</code>). \n",
    "\n",
    "We will begin be importing numpy and matplotlib utilities, storing our data, and getting a visualization of what this data represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data sizes:\n",
      "Three data:\n",
      "\tT: 700 | Vector Length: 64\n",
      "Five data:\n",
      "\tT: 700 | Vector Length: 64\n",
      "\n",
      "\n",
      "Examples of some data:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC+CAYAAACWL9wvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAE7ElEQVR4nO3dsXEUSxRA0ZWKICj875MERQRESQSUksDHp34Uu9/4JtCLZrdvz2jPcYWk0dNwq41XrafL5XICoPG8+gEAHonoAoREFyAkugAh0QUIvRt98NPzl02rDd9+ft/2NIs8v//x9Lf/dutMrtnbzF4zk9PJu/I753//2dVq0OcPH6d83Zfz1+n/f1a8J7fMazQTJ12AkOgChEQXICS6ACHRBQiJLkBouDI2MmuFY7SmcbT1Iv634l0Z2cN7NGt9a/SzjT4263kqR3oXnHQBQqILEBJdgJDoAoREFyAkugChzStjI9fWN7autdzyPUdezps/9dBGM7vnTFasR93iXnM50hrWtVmu/lluacpWs2bipAsQEl2AkOgChEQXICS6ACHRBQiJLkBoyp7uiuvS9r5nyO8d7a+87t2sq1H3cB3m3mydiZMuQEh0AUKiCxASXYCQ6AKERBcgtHll7GjX9e3dkf6a6T29pSv79u4t/6XtFb8zK2MAByC6ACHRBQiJLkBIdAFCogsQmnLL2DUr/uLv3s1Y2XnL87qFuawzes8f5ffipAsQEl2AkOgChEQXICS6ACHRBQhtXhmbdSvRW10buWVeb3Ump9PjvkezVqeOfFvYinWyFfNy0gUIiS5ASHQBQqILEBJdgJDoAoREFyA03NN1BeOvHnWv9Bpz+dXWmazYHb1lzi/nOz7IH+xx/3g0s9FMnHQBQqILEBJdgJDoAoREFyAkugChp8vlsvoZAB6Gky5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNC70Qc/PX+5zPim335+n/FlN3t+/+Ppb//trJlsNWuWr5nJ6eRd+Z0V78qseX3+8PGPH3s5f33ImYyM3hMnXYCQ6AKERBcgJLoAIdEFCIkuQGi4MrbCaDVllpdz/i1fZevKyy2z3PtMaO1tdW8Pxmt0f/48J12AkOgChEQXICS6ACHRBQiJLkBoysqY9ZLOihW7e7rlXXnU9cIVK4QjxUxW3J42i5MuQEh0AUKiCxASXYCQ6AKERBcgJLoAod1d7fiotu4hjj7vCDu8157xEXe+Z/3MR9uJLqz4/+OkCxASXYCQ6AKERBcgJLoAIdEFCE1ZGbtlDejoK1B7cm1F6J7z3Nt1g0e2YiZ7X82bNZOtLTqdtj+Tky5ASHQBQqILEBJdgJDoAoREFyC05Jax0arFo66TzXj+aysv1ZrQ0X833M/eVgu3tugWTroAIdEFCIkuQEh0AUKiCxASXYDQcGXsLa9obWUmr7f3G6xm2dt61JF/D7Nu/FoxEyddgJDoAoREFyAkugAh0QUIiS5ASHQBQpuvdjzyzt8tVlwFN8voZ3k53+9rHW0u9zLjCtNZqh3zWe/Jkd4xJ12AkOgChEQXICS6ACHRBQiJLkDo6XK5rH4GgIfhpAsQEl2AkOgChEQXICS6ACHRBQj9B+SEP3pS2ymiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from each file as ndarray (int)\n",
    "with open(\"train3_oddYr.txt\") as three_data_file:\n",
    "    three_data = np.loadtxt(three_data_file, dtype='int')\n",
    "    \n",
    "with open(\"train5_oddYr.txt\") as five_data_file:\n",
    "    five_data = np.loadtxt(five_data_file, dtype='int')\n",
    "\n",
    "print(\"\\nData sizes:\")\n",
    "print(\"Three data:\")\n",
    "print(\"\\tT: %s | Vector Length: %s\"%(np.shape(three_data)[0], np.shape(three_data)[1]))\n",
    "print(\"Five data:\")\n",
    "print(\"\\tT: %s | Vector Length: %s\\n\\n\"%(np.shape(five_data)[0], np.shape(five_data)[1]))\n",
    "\n",
    "# Plot five members of each data set for sanity check\n",
    "print(\"Examples of some data:\\n\")\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "    if i < 5:\n",
    "        image = three_data[i,:].reshape((8,8))\n",
    "        sub = fig.add_subplot(2,5,i+1)\n",
    "        plt.axis('off')\n",
    "        sub.imshow(image)\n",
    "    else:\n",
    "        image = five_data[i,:].reshape((8,8))\n",
    "        sub = fig.add_subplot(2,5,i+1)\n",
    "        plt.axis('off')\n",
    "        sub.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already in just the first 10 numbers, we see 3's and 5's that are ambiguously represented. Hopefully, if we train our model well enough, it will be able to correctly classify most (but probably not all) of these. \n",
    "\n",
    "# Key functions\n",
    "\n",
    "Below we define a few functions that we will use in our algorithm. \n",
    "- <code>compLogLik(w, x, y)</code>: takes a weight vector, and as many data vectors (x) as desired, and computes the log likelihood that each data vector equals 1.\n",
    "- <code>gradAscent(w, x, y, num_iterations)</code>: Performs gradient ascent on the log likelihood function, by differentiating it with respect to w, and updating w in the direction of the ascent. It will perform the algorithm *num_iterations* times\n",
    "- <code>sigmoid(w, x)</code>: computes the sigmoid of the dot product between w and a single data vector x\n",
    "- <code>dL_dw(w, ith_w, x, y)</code>: computes the gradient of the log likelihood with respect to the *ith_w* weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compLogLik(w, x, y):\n",
    "    # w is weight vector\n",
    "    # x is entire matrix of T data vectors\n",
    "    # y is vector of length T, =0 if three_data or =1 if five_data\n",
    "    sumLogLik = 0\n",
    "    \n",
    "    for i in range(np.shape(x)[0]):\n",
    "        sumLogLik += np.log(sigmoid(w, x[i,:])**(y[i]) * sigmoid(-1*w, x[i,:])**(1-y[i]))\n",
    "    \n",
    "    return sumLogLik\n",
    "\n",
    "def gradAscent(w, x, y, num_iterations):\n",
    "    \n",
    "    LogLikProgression = []\n",
    "    \n",
    "    LogLikProgression.append(compLogLik(w, x, y))\n",
    "    print(\"Log Likelihood initial: %s\"%(LogLikProgression[0]))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        derivatives = []\n",
    "\n",
    "        for j in range(len(w)):\n",
    "            derivatives.append(dL_dw(w, j, x, y))\n",
    "        \n",
    "        for k in range(len(w)):\n",
    "            w[k] = w[k] + .2/(np.shape(x)[0]) * derivatives[k]\n",
    "        \n",
    "        LogLikProgression.append(compLogLik(w, x, y))\n",
    "        \n",
    "    return w, LogLikProgression\n",
    "    \n",
    "    \n",
    "def sigmoid(w, x):\n",
    "    # w is the weight vector\n",
    "    # x is a single data vector\n",
    "    w_dot_x = np.dot(w,x)\n",
    "    sigm = 1/(1 + np.exp(-1*w_dot_x))\n",
    "    return sigm\n",
    "\n",
    "def dL_dw(w, ith_w, x, y):\n",
    "    # ith_w is which weight we are taking derivative to \n",
    "    \n",
    "    sum_dL_dw = 0\n",
    "    \n",
    "    for ii in range(len(x)):\n",
    "        sum_dL_dw += x[ii, ith_w] * (y[ii] - sigmoid(w, x[ii, :]))\n",
    "    \n",
    "    return sum_dL_dw\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all of the functions we need to run the algorithm!\n",
    "\n",
    "# Running the algorithm\n",
    "\n",
    "We can now reformat our *three_data* and *five_data* matrices, and create a corresponding vector of *y*, each entry of which corresponds to the same entry in our *training_data* vector, and is a 0 if the vector comes from the three data set, and a 1 if the vector comes from the 5 data set. \n",
    "\n",
    "We will initialize our weights randomly using a normal distribution. How these are initalized doesn't matter too much, because we will be iteratively updating them as our algorithm converges on the maximum log likelihood.\n",
    "\n",
    "Below, the algorithm is run for 100 iterations, and the convergence of the value of the log likelihood is plotted for each iteration of the algorithm, to show that it is converging as the weights are becoming more and more optimized. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood initial: -2674.931269801083\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxddZ3/8denSbMvTZq0SZvuTeneQkNBNhkWqYJUQRSVoQpScXBjxhlBxnHcRp0RGRUF+QECilRkLZtIEQSVLS2FbrRN97TZkzZbkzTJ5/fHPc1c2pQuN+lN7n0/H4/7uOd+z7nJ53DCefd8v2cxd0dERARgSLQLEBGRgUOhICIiPRQKIiLSQ6EgIiI9FAoiItIjMdoFRCovL8/Hjx8f7TJERAaV5cuX17p7/oHtgz4Uxo8fT2lpabTLEBEZVMxsW2/t6j4SEZEeCgUREemhUBARkR79Fgpm9p9mttPMVgavD4XNu9HMysxsvZldENa+IGgrM7Mb+qs2ERHpXX8PNN/i7j8ObzCz6cDlwAxgFLDMzKYEs38BnA+UA2+Y2VJ3X9vPNYqISCAaZx8tBJa4ezuwxczKgPnBvDJ33wxgZkuCZRUKIiLHSX+PKXzRzN42s7vNLCdoGw3sCFumPGg7VPtBzGyxmZWaWWlNTU1/1C0iEpciOlIws2VAQS+zbgJuA74LePB+M3AVYL0s7/QeUL3e19vd7wDuACgpKdG9v0UkZu3r6qahpYPa5g7qWtqpD6brW9pZfOYkstOG9unviygU3P28I1nOzP4f8GTwsRwYEza7CNgVTB+qXUQkZuzt6KK2uZ3qpnZqm0OvuuaOnuna5g7qmtupa+lgd+u+Xn9GwhBj4dzRAysU3ouZFbp7RfDxo8DqYHop8Dsz+wmhgeZi4HVCRxDFZjYB2EloMPpT/VWfiEhf6u526lo6qG5qo7qpnZreXs2h9+b2zl5/RlZKInmZyeSlJ3NCQSZ5GcnkpieRl5HM8PQkctOTGJ6RTF5GElkpQxkypLeOl8j050Dzf5vZXEJdQFuBzwO4+xoze5DQAHIncJ27dwGY2ReBZ4EE4G53X9OP9YmIHFZ3t1Pf2kFVYxvVje1UNbZR1dhOVVMb1Y2hAKhuDO3wu7oP7s3OTEkkPyOZ/MxkZozKIi+Yzs9IJi8zifyMFPIyQzv85MSEKKzhu9lgfxxnSUmJ695HInIsOru6qWpqp3LPXir2tFEZvCoa26ja00bFnjaqm9rY13XwfnJ4ehL5mcmMyEphZGYyI7KSGZGZwoiw6fzMZFKGRn9H3xszW+7uJQe2D/ob4omI9GZ/d86u3XvZtXsvO3fvZdfuNiobQ+8Ve/ZS09TOgf+4Tx2aQEF2CiOzkpk/IZeRWSkUZCVTkJ0SCoCsFPIzkklKjM0bQigURGRQ6uzqpmJPG+UNoR1+eUNrz85/Z8Nedu1po6Oz+13fSR2aQOGwFEZlp3JWcT6Fw1IpzE4JXqkUZKWQlZqIWd/31Q8WCgURGZDcnZqmdrbXt7K9vpUd9XvZ0dBKeUNourKx7aA+/BGZyYzOSWXm6GwumFHAqGGpjB6W2hMEw9KGxvUO/0goFEQkato7u9hRv5ft9S1srd2/829lW/DefsC/9EdmJTMmJ42Tx+dQlJPGmNxUinLSenb8A2GgdrBTKIhIvwrt+FvZUtvK1toWttS1sK0uFAK79uwl/FyX9KQExg5PZ1J+OudMHcGY3DTG5KQyJje04x+og7axRKEgIhFzd6qb2tlU3cym2hY21zSzpbaFzTUtlDe0vmswd1jaUMYPT+fk8TmMHV7E+OFpjBuezrjhaQxPT1L3TpQpFETkiHV2dbO9vpWN1c2UVTeHQqCmmU01Le+6ICt1aAIT8tKZXZTNR+aOYnxeOhOC17C0pCiugRyOQkFEDtLV7Wyra2FDVRPrK5vZUN1EWVXoX/8dXf/Xz1+QlcLkERlcetJoJo3IYGJeBhPz0ynISumXq22l/ykUROLY/m6fdRWNrK9sCr2qmthY3dxzOqcZjMlJo3hEBmdPzad4RCaTR2QwKT+dzJS+ve+ORJ9CQSROdHR2U1bdzNqKRtaFvRrCbrhWkJXClIJMTp+cx5SRmUwZmcHkERmkJWlXES+0pUVi0N6OLtZWNLJm1x5W79zDml2NbKhq6rldQ3LiEKYWZHLBjAKmFmQytTCLqQWZ6u8XhYLIYNe2r4s1uxpZVb6bVTsbWbVzN2XVzT1n/OSmJzFjVBZXnTGBGaOymV6Yyfjh6SQmxOZtGiQyCgWRQaSr2ymrbmbljgZW7tjDWzt2s6Gqic4gAfIykpldlM2CmYXMHJXFzNHZFGan6DRPOWIKBZEBrKGlgxXbG3hz+25WbG/g7fI9Pad+ZqYkMqdoGIvPmsjsomHMHTOMkVnJCgCJiEJBZIBwdzbXtlC6tZ7SrQ0s397A5poWIPSUrWmFmXz0xNHMHTOMuWOHMWF4uk77lD6nUBCJkq5uZ11FI69tqef1LXWUbm2grqUDgJy0ocwbl8PH5hVx0tgcZhdl6wwgOS70VyZynHR1O2t27eGVTXW8ujkUAk1BV9CY3FTef0I+88fnUjI+l0n56eoGkqhQKIj0E3dnY3Uzfyur5W9ldby2pY6mtlAITMxP58NzR3HKhFzmT8ilMDs1ytWKhCgURPpQdWMbL2+s5a9loVdNUzsAY3PTuGh2IadOHM77Jg5nRFZKlCsV6Z1CQSQCHZ3dLN/WwIsbqvnL+hreqWwCQs/vPW1yHmdMHs5pk/IYk5sW5UpFjoxCQeQoVTW28eL6al54p4a/ltXS3N7J0ASjZFwuX18wlTOL85hemKUzg2RQUiiIHIa7s2ZXI8vWVfH8umpW7dwDwKjsFC6eO4qzp+Rz2uQ8MpL1v5MMfvorFunFvq5uXt9Sz5/WVPLc2ip27WnDDE4am8O/XnAC504bwQkjM3WGkMQchYJIoL2zi79urOWZ1ZUsW1fF7tZ9pAwdwpnF+Xz1/CmcO3UEwzOSo12mSL9SKEhca+/s4uUNtTy1qoJla6toau8kMyWR86eN5IKZBZxVnE9qkp4LLPFDoSBxp6vb+fumWp54axd/XF1JY1sn2alDWTCzgA/NLuT0SXkkJeoOohKfFAoSF9yd1TsbefTNnTzx9i5qmtrJSE7kAzNG8uHZozh9soJABBQKEuMq97Tx6Js7eWRFORurm0lKGMI/TM1n4dzRnDN1BClD1TUkEk6hIDGnvbOLZWurebB0By9vrKHboWRcDv/10VlcOKuQ7DQ9V1jkUBQKEjPKqpt44PUdPLyinN2t+xiVncIX/2Eyl5xUxPi89GiXJzIoKBRkUGvv7OKZVZXc/9o23tjawNAE4wPTC/j4yWM4Y3IeCbqqWOSoKBRkUNpR38pvX9vGH0rLqW/pYPzwNL7xoalcclIRebqWQOSYKRRk0HB3XtlUxz1/38qydVWYGedNG8EVp47j9El5uteQSB9QKMiA197ZxdKVu7jrr1t4p7KJnLShXPv+SVxx6jhGDdNzCET6kkJBBqw9rfv47Wvb+PXftlLb3M4JIzP50aWzWDh3tE4lFeknCgUZcCr27OXOl7fwwOvbae3o4qwp+Sw+cyKnTx6uG9CJ9DOFggwYW2tbuP0vm3h4RTndDhfPGcU1Z05k+qisaJcmEjciCgUzuwz4T2AaMN/dS8Pm3QhcDXQBX3b3Z4P2BcBPgQTgTnf/YdA+AVgC5AIrgH90945I6pPBYVNNM7f+uYzHV+4kMWEIl588lsVnTdTTykSiINIjhdXAJcCvwhvNbDpwOTADGAUsM7MpwexfAOcD5cAbZrbU3dcCPwJucfclZnY7oUC5LcL6ZADbXNPMz57fyNK3dpGcmMDVZ0zgmjMn6vnFIlEUUSi4+zqgt37ehcASd28HtphZGTA/mFfm7puD7y0BFprZOuAc4FPBMvcSOgJRKMSgHfWt/Oz5jTy8opzkxASuOXMi15w1UdcXiAwA/TWmMBp4NexzedAGsOOA9lOA4cBud+/sZfmDmNliYDHA2LFj+6hk6W91ze38/M9l3P/aNsyMz54+gS+cPUlhIDKAHDYUzGwZUNDLrJvc/fFDfa2XNgd6uzexv8fyvXL3O4A7AEpKSg65nAwMLe2d3PnyFu54aRNtnd18vGQMXz53MoXZusZAZKA5bCi4+3nH8HPLgTFhn4uAXcF0b+21wDAzSwyOFsKXl0Gqq9t5eHk5P/7Teqqb2vngzAK+dsEJTMrPiHZpInII/dV9tBT4nZn9hNBAczHwOqEjguLgTKOdhAajP+XubmYvAB8jdAbSIuBQRyEyCPx9Uy3ffXId6yoaOXHsMG67Yh7zxuVEuywROYxIT0n9KPBzIB94ysxWuvsF7r7GzB4E1gKdwHXu3hV854vAs4ROSb3b3dcEP+7rwBIz+x7wJnBXJLVJdJQ3tPJfT6/j6VWVFOWkcuunTuTCWYW66ExkkDD3wd0lX1JS4qWlpYdfUPpV274u7nhpM794oQwzuO7syVxz1kTdjkJkgDKz5e5ecmC7rmiWiL20oYb/eHw1W+tauXBWITddOE03qhMZpBQKcsyqm9r4zhNrefLtCibkpfObq+dzZnF+tMsSkQgoFOSouTsPlu7g+0+to21fN9efN4Vrz55IcqK6ikQGO4WCHJXtda18/eG3eWVzHfMn5PKDS2bpFFORGKJQkCPS3e385tVt/PCZd0gYYvzgkll8omSMnnYmEmMUCnJY5Q2tfO0Pb/Hq5nrePyWfH1wySwPJIjFKoSCH5O48tLycbz+xFoD/vnQ2l5UU6ZoDkRimUJBeNbR0cOMjq/jjmkrmT8jl5svm6PkGInFAoSAHeWVTHdf/fiV1Le3c+MGpfO7MiSRo7EAkLigUpEdnVzc/fX4jt75Qxvjh6Txy5enMKsqOdlkichwpFASA6sY2vvTAm7y2pZ6PzSvi2xfPID1Zfx4i8Ub/1wt/K6vlK0vepKW9i5svm8Ol84qiXZKIRIlCIY65O7f9ZRM/fnY9k/IzeOCakygemRntskQkihQKcaqpbR9f+8NbPLumiotmF/KjS2eru0hEFArxaEttC5+79w221rXy7xdO4+ozJujaAxEBFApx5+WNNVx3/woShhi/vfoU3jdpeLRLEpEBRKEQJ9yde/6+le8+uZbiEZncuahEF6OJyEEUCnGgs6ub7zy5lvte2cb500dyyyfmkqHxAxHphfYMMa65vZMv/W4FL6yvYfFZE7lhwVTd2VREDkmhEMOqG9v4zK/fYH1VE9//6Ew+fcq4aJckIgOcQiFGbapp5sq7XqehtYO7FpVw9gkjol2SiAwCCoUYtGJ7A1ff8wZDzFiy+FRmFw2LdkkiMkgoFGLMSxtq+PxvljMiK5l7Pzuf8Xnp0S5JRAYRhUIM+ePqCr78wEomjcjgvqvmk5+ZHO2SRGSQGRLtAqRvPLS8nH+6fwUzR2ex5JpTFQgickx0pBAD7n9tGzc9upozi/P41T/OIy1Jm1VEjo32HoPcfa9s5T8eX8M5U0fwy0+fRMrQhGiXJCKDmEJhELvrr1v47pNrOX/6SH7xqZNISlRvoIhERqEwSP36b6FA+ODMAn72yRMZmqBAEJHIaU8yCP321W18+4m1XDBjpAJBRPqU9iaDzINv7ODfH1vNOVNH8PNPnqRAEJE+pT3KIPLEW7v4+iNvc9aUfH75aY0hiEjf015lkHhxfTXX/34lJ4/L5VdXzNNZRiLSLxQKg8AbW+u59rfLOaEgkzs/U0JqkgJBRPqHQmGAe6eykavueYNR2ance9V8slKGRrskEYlhCoUBbOfuvSy6+3XSkxL5zedOIS9Dt64Qkf6lUBigdrd28Jm7X6e1vYt7rjqZ0cNSo12SiMSBiELBzC4zszVm1m1mJWHt481sr5mtDF63h82bZ2arzKzMzH5mZha055rZc2a2MXjPiaS2waxtXxeL71vOtrpWfnXlPKYWZEW7JBGJE5EeKawGLgFe6mXeJnefG7yuDWu/DVgMFAevBUH7DcDz7l4MPB98jjvuzr899Davb63nxx+fw2mT8qJdkojEkYhCwd3Xufv6I13ezAqBLHd/xd0duA/4SDB7IXBvMH1vWHtc+enzG1n61i7+9YITuHjOqGiXIyJxpj/HFCaY2Ztm9hczOzNoGw2Uhy1THrQBjHT3CoDg/ZAPFTazxWZWamalNTU1/VF7VDy+cif/u2wjl55UxD+dPSna5YhIHDrsDfHMbBlQ0Musm9z98UN8rQIY6+51ZjYPeMzMZgDWy7J+xNXu/4L7HcAdACUlJUf9/YFoxfYG/vUPb3PKhFx+cMksgqEWEZHj6rCh4O7nHe0Pdfd2oD2YXm5mm4AphI4MisIWLQJ2BdNVZlbo7hVBN1P10f7ewapyTxuf/81yCrJTuP2Kebp9hYhETb/sfcws38wSgumJhAaUNwfdQk1mdmpw1tGVwP6jjaXAomB6UVh7TGvb18Xnf1NKa3sndy4qISc9KdoliUgci/SU1I+aWTnwPuApM3s2mHUW8LaZvQU8BFzr7vXBvC8AdwJlwCbgmaD9h8D5ZrYROD/4HNPcnW88soq3yvdwyyfmMmVkZrRLEpE4F9FDdtz9UeDRXtofBh4+xHdKgZm9tNcB50ZSz2Bz79+38sibO7n+vCl8YEZvwzYiIseXOq+jpHRrPd97ah3nTRvJl86ZHO1yREQAhUJU1DS1c93vVjA6J5WbPz6HIUN0ppGIDAwKheOss6ubLz2wgj1793H7FfPITtVdT0Vk4IhoTEGO3i3LNvDq5np+8vE5TCvUPY1EZGDRkcJx9NKGGn754iYuP3kMl5xUdPgviIgcZwqF46S6sY3rf7+S4hEZfOvDM6JdjohIr9R9dBx0dTtfWbKSlo5OlnzqVD1OU0QGLIXCcXDbi2W8srmO/750NsW6QE1EBjB1H/WzlTt287/LNvLhOaO4rETjCCIysCkU+lFLeydfXfImI7NS+N5HZurOpyIy4Kn7qB9954m1bKtv5YFrTtX1CCIyKOhIoZ88u6aS35fu4Avvn8SpE4dHuxwRkSOiUOgHdc3tfOORVcwYlcVXz5sS7XJERI6Yuo/6mLvz74+tpqmtk99dM1cPzBGRQUV7rD629K1dPLO6kuvPn8IJBTr9VEQGF4VCH6pubOM/Hl/DiWOHsfisidEuR0TkqCkU+oi7c9Njq2nb18XNl80hQbfDFpFBSKHQR55eVclza6v45/OnMDE/I9rliIgcE4VCH9jd2sG3lq5m1uhsrj5jQrTLERE5Zjr7qA9876l1NLTu496r5pOYoJwVkcFLe7AIvbyxhoeWl3Pt+ycyY1R2tMsREYmIQiECbfu6+OZjq5mQl86XzimOdjkiIhFT91EEbv/LJrbWtfLbq08hZaiekSAig5+OFI7R1toWfvniJj48ZxRnFOdFuxwRkT6hUDgG7s43H19NcsIQvnnhtGiXIyLSZxQKx+CZ1ZW8vLGWf/nAFEZkpUS7HBGRPqNQOEpt+7r4/lPrmFaYxT++b3y0yxER6VMKhaN011+3sHP3Xr550TTdykJEYo5C4ShUN7XxyxfK+MD0kZw2SYPLIhJ7FApH4eZnN9DR1c03PqTBZRGJTQqFI7R65x4eXL6Dz5w2nvF56dEuR0SkXygUjtDNf1pPdupQvqgrl0UkhikUjsDb5bt5YX0N15w5kezUodEuR0Sk3ygUjsDP/1xGVkoiV75vXLRLERHpVwqFw1i7q5Hn1lZx1RkTyEzRUYKIxDaFwmHc+sJGMpMT+expeniOiMQ+hcJ72FDVxNOrKvnM6ePJTtNRgojEvohCwcz+x8zeMbO3zexRMxsWNu9GMyszs/VmdkFY+4KgrczMbghrn2Bmr5nZRjP7vZklRVJbX7jr5S2kJSVw1ek6ShCR+BDpkcJzwEx3nw1sAG4EMLPpwOXADGAB8EszSzCzBOAXwAeB6cAng2UBfgTc4u7FQANwdYS1RaS9s4unV1ewYGYBOelRzycRkeMiolBw9z+5e2fw8VWgKJheCCxx93Z33wKUAfODV5m7b3b3DmAJsNDMDDgHeCj4/r3ARyKpLVIvrq+hqa2Ti+eMimYZIiLHVV+OKVwFPBNMjwZ2hM0rD9oO1T4c2B0WMPvbe2Vmi82s1MxKa2pq+qj8d1v61i5y05M4fbLucSQi8eOwoWBmy8xsdS+vhWHL3AR0Avfvb+rlR/kxtPfK3e9w9xJ3L8nPzz/cKhy15vZOlq2t4sJZhQxN0Fi8iMSPwz6j2d3Pe6/5ZrYIuAg4193378jLgTFhixUBu4Lp3tprgWFmlhgcLYQvf9w9t7aS9s5uLp6rriMRiS+Rnn20APg6cLG7t4bNWgpcbmbJZjYBKAZeB94AioMzjZIIDUYvDcLkBeBjwfcXAY9HUlsklq7cxajsFOaNzYlWCSIiURFp38itQCbwnJmtNLPbAdx9DfAgsBb4I3Cdu3cFRwFfBJ4F1gEPBstCKFz+2czKCI0x3BVhbcekvqWDlzfW8uE5oxiih+iISJw5bPfRe3H3ye8x7/vA93tpfxp4upf2zYTOToqqp1dV0Nnt6joSkbikUdQDvLyxhjG5qUwvzIp2KSIix51C4QDrKpqYNTqb0KUTIiLxRaEQprm9k+31rUwr0FGCiMQnhUKY9ZVNAExV15GIxCmFQph1FY0ATC3IjHIlIiLRoVAI805lI5nJiRTlpEa7FBGRqFAohHmnoomphZkaZBaRuKVQCLg771Q2MVWDzCISxxQKgfKGvTS3dzJNg8wiEscUCoGeQeZCDTKLSPxSKATeqWzCDE4YqVAQkfilUAisq2hkXG4a6ckR3Q5KRGRQUygENMgsIqJQAKC1o5OtdS0aTxCRuKdQIHR7C3d05pGIxD2FAqGuI0A3whORuKdQAN6paCQ9KUG3txCRuKdQIHTh2rjh6Xr8pojEPYUCUN/awfCMpGiXISISdQoFoKGlg5w0hYKIiEIBqG/pIDddoSAiEvehsK+rm8a2Th0piIigUKChtQOA3PShUa5ERCT6FAot+wDITU+OciUiItEX96FQ3xI6UsjRkYKIiELh/7qPNKYgIhL3obD/SCFXA80iIgqFhiAUhikUREQUCvWtHWQmJ5KUGPf/KUREFAoNLR3kaDxBRARQKFDfuk+hICISiPtQaGjpIDdNp6OKiIBCgXp1H4mI9Ij7UGho7dDpqCIigbgOhbZ9XbR2dOlIQUQkENehoKuZRUTeLa5Doee+R+o+EhEBIgwFM/sfM3vHzN42s0fNbFjQPt7M9prZyuB1e9h35pnZKjMrM7OfmZkF7blm9pyZbQzecyJbtcPbf4dUPYpTRCQk0iOF54CZ7j4b2ADcGDZvk7vPDV7XhrXfBiwGioPXgqD9BuB5dy8Gng8+96v6Vh0piIiEiygU3P1P7t4ZfHwVKHqv5c2sEMhy91fc3YH7gI8EsxcC9wbT94a195v99z3SmIKISEhfjilcBTwT9nmCmb1pZn8xszODttFAedgy5UEbwEh3rwAI3kcc6heZ2WIzKzWz0pqammMuuK6lAzPITtXFayIiAImHW8DMlgEFvcy6yd0fD5a5CegE7g/mVQBj3b3OzOYBj5nZDMB6+Tl+tEW7+x3AHQAlJSVH/f39Glo6GJY6lIQhvZUlIhJ/DhsK7n7ee803s0XARcC5QZcQ7t4OtAfTy81sEzCF0JFBeBdTEbArmK4ys0J3rwi6maqPdmWOVn2rrmYWEQkX6dlHC4CvAxe7e2tYe76ZJQTTEwkNKG8OuoWazOzU4KyjK4HHg68tBRYF04vC2vtN6L5HCgURkf0Oe6RwGLcCycBzwZmlrwZnGp0FfMfMOoEu4Fp3rw++8wXgHiCV0BjE/nGIHwIPmtnVwHbgsghrO6z6lg7G5Kb1968RERk0IgoFd598iPaHgYcPMa8UmNlLex1wbiT1HK2G1g7mFA07nr9SRGRAi9srmt2dhhY9S0FEJFzchkJLRxcdXd3kput0VBGR/eI2FBp03yMRkYPEbSjU62pmEZGDxG8o6LbZIiIHidtQ0H2PREQOFreh0PMsBYWCiEiPuA2FhtYOEocYmcmRXr8nIhI74jYU6oNrFIIrsUVEhDgOBd33SETkYHHbdzKrKJsJ+enRLkNEZECJ21C47h96vW2TiEhci9vuIxEROZhCQUREeigURESkh0JBRER6KBRERKSHQkFERHooFEREpIdCQUREepi7R7uGiJhZDbDtGL+eB9T2YTmDgdY5PmidY1+k6zvO3fMPbBz0oRAJMyt195Jo13E8aZ3jg9Y59vXX+qr7SEREeigURESkR7yHwh3RLiAKtM7xQesc+/plfeN6TEFERN4t3o8UREQkjEJBRER6xG0omNkCM1tvZmVmdkO06+lrZjbGzF4ws3VmtsbMvhK055rZc2a2MXjPiXatfc3MEszsTTN7Mvg8wcxeC9b592YWU89hNbNhZvaQmb0TbO/3xfp2NrPrg7/r1Wb2gJmlxNp2NrO7zazazFaHtfW6XS3kZ8H+7G0zO+lYf29choKZJQC/AD4ITAc+aWbTo1tVn+sE/sXdpwGnAtcF63gD8Ly7FwPPB59jzVeAdWGffwTcEqxzA3B1VKrqPz8F/ujuU4E5hNY9ZrezmY0GvgyUuPtMIAG4nNjbzvcACw5oO9R2/SBQHLwWA7cd6y+Ny1AA5gNl7r7Z3TuAJcDCKNfUp9y9wt1XBNNNhHYUowmt573BYvcCH4lOhf3DzIqAC4E7g88GnAM8FCwSU+tsZlnAWcBdAO7e4e67ifHtTOhRwqlmlgikARXE2HZ295eA+gOaD7VdFwL3ecirwDAzKzyW3xuvoTAa2BH2uTxoi0lmNh44EXgNGOnuFRAKDmBE9CrrF/8L/BvQHXweDux2987gc6xt64lADfDroMvsTjNLJ4a3s7vvBH4MbCcUBnuA5cT2dt7vUNu1z/Zp8RoK1ktbTJ6ba2YZwMPAV929Mdr19Cczuwiodvfl4c29LBpL2zoROAm4zd1PBFqIoa6i3gT96AuBCcAoIJ1Q98mBYmk7H06f/Z3HayiUA2PCPhcBu6JUS78xs6GEAuF+d38kaK7af2kXyJUAAAF3SURBVFgZvFdHq75+cDpwsZltJdQleA6hI4dhQTcDxN62LgfK3f214PNDhEIilrfzecAWd69x933AI8BpxPZ23u9Q27XP9mnxGgpvAMXB2QpJhAaplka5pj4V9KXfBaxz95+EzVoKLAqmFwGPH+/a+ou73+juRe4+ntA2/bO7fxp4AfhYsFisrXMlsMPMTgiazgXWEsPbmVC30almlhb8ne9f55jdzmEOtV2XAlcGZyGdCuzZ3810tOL2imYz+xChf0UmAHe7+/ejXFKfMrMzgJeBVfxf//o3CI0rPAiMJfQ/12XufuBg1qBnZmcDX3P3i8xsIqEjh1zgTeAKd2+PZn19yczmEhpYTwI2A58l9A++mN3OZvZt4BOEzrJ7E/gcoT70mNnOZvYAcDahW2RXAd8CHqOX7RqE462EzlZqBT7r7qXH9HvjNRRERORg8dp9JCIivVAoiIhID4WCiIj0UCiIiEgPhYKIiPRQKIiISA+FgoiI9Pj/a+NSs3QzT+0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data = np.array([(three_data[i], five_data[i]) for i in range(len(three_data))]).reshape((len(three_data)*2, 64))\n",
    "y = np.array([(0,1) for i in range(len(three_data))]).reshape((len(three_data)*2))\n",
    "w = np.random.randn(np.shape(three_data)[1])\n",
    "\n",
    "W, LOGLIKPROG = gradAscent(w, training_data, y, 100)\n",
    "\n",
    "fig = plt.plot(LOGLIKPROG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classifications correct: 350/400\n",
      "Accuracy: 0.875\n",
      "Learned weights: \n",
      "[[-2.05093318  0.56552483 -0.48921476 -0.7190121   0.59357329  0.02666997\n",
      "   2.2525402   0.59048552]\n",
      " [-0.68280782  0.95945594 -0.01273241 -0.29001004  1.43672687 -0.16219065\n",
      "   0.68031742 -0.93659212]\n",
      " [ 1.33745048 -0.19797059  1.4169371   0.35883485  0.24693447 -1.2288639\n",
      "  -2.03572699  0.37465784]\n",
      " [-0.06037266 -0.1505419   0.23883401  1.44973183 -1.77659121 -1.25486495\n",
      "  -0.54770351 -0.5302408 ]\n",
      " [ 1.652057    1.35774565 -0.41046041  0.82669373 -1.29629419  0.19091118\n",
      "   1.35482634 -0.54174969]\n",
      " [-0.51655544  0.39524037 -1.34026041  0.3080257  -0.87723574 -1.01451297\n",
      "  -1.13331699  0.18210762]\n",
      " [-1.11763483  1.30009216  0.3213246  -0.10866501  1.64303644 -1.79641991\n",
      "  -0.03580032 -0.08077918]\n",
      " [-0.85536469  2.43709864 -0.77255847  0.37046613 -0.89591672 -0.49895896\n",
      "  -0.75656349  0.29823102]]\n"
     ]
    }
   ],
   "source": [
    "def compProb(w, x):\n",
    "    \n",
    "    probs = []\n",
    "    \n",
    "    for data_vec in range(np.shape(x)[0]):\n",
    "        probs.append(sigmoid(w, x[data_vec,:]))\n",
    "    \n",
    "    return probs\n",
    "    \n",
    "\n",
    "# Load the test data so that we can see how well our algorithm works\n",
    "with open(\"test3_oddYr.txt\") as three_test_data_file:\n",
    "    three_test_data = np.loadtxt(three_test_data_file, dtype='int')\n",
    "    \n",
    "with open(\"test5_oddYr.txt\") as five_test_data_file:\n",
    "    five_test_data = np.loadtxt(five_test_data_file, dtype='int')\n",
    "\n",
    "test_data = np.array([(three_test_data[i], five_test_data[i]) for i in range(len(three_test_data))]).reshape((len(three_test_data)*2, 64))\n",
    "y_test_data = np.array([(0,1) for i in range(len(three_test_data))]).reshape((len(three_test_data)*2))\n",
    "\n",
    "probs = compProb(W, test_data)\n",
    "\n",
    "rounded_probs = [np.around(probs[ij]) for ij in range(len(probs))]\n",
    "\n",
    "correct = [int(c1)&r1 for c1, r1 in zip(rounded_probs, y_test_data)]\n",
    "\n",
    "print(\"Number of classifications correct: %s/%s\"%(np.sum(correct), len(three_test_data)))\n",
    "\n",
    "print(\"Accuracy: %s\"%(np.sum(correct)/len(three_test_data)))\n",
    "\n",
    "print(\"Learned weights: \")\n",
    "print(W.reshape((8,8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
